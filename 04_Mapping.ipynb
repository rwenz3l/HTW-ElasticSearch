{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment 04\n",
    "Für die Realisierung der Facettensuche von Medien muss das aktuelle Medium-Mapping von HTWplus angepasst werden.\n",
    "Dazu benötigt es neue Felder, mit den Werten, die hinterher aggregiert werden sollen.\n",
    "\n",
    "#### Aufgabe\n",
    "Erweitere das **Medium-Mapping** so, damit folgende Aggregationen ausgeführt werden können:\n",
    "- 1) Eine Gruppierung nach Eigentümern (Vor- und Nachname) **(2 Punkte)** ✓\n",
    "    - Hinzugeüft wurde \"ownername\" zu \"medium\" vom type \"keyword\"\n",
    "    - Im Invertierten Index steht so der Komplette Name als 1 Wort.  \n",
    "\n",
    "\n",
    "- 2) Eine Gruppierung nach Speicherort (Root-Folder). **(2 Punkte)** ✓\n",
    "    - Dem Index wurde ein filepath_analyzer hinzugefügt\n",
    "    - Spaltet `path/to/file/` demnach in [\"path/\", \"path/to/\", \"path/to/file/\"]\n",
    "    - Dem medium mapping wurde ein `filepath` hinzugefügt vom type text\n",
    "    - Wird beim indexieren + suchen mit dem custom filepath_analyzer bearbeitet  \n",
    "\n",
    "\n",
    "- 3) Eine Gruppierung nach Upload-Datum. Flexibel nach Tag, Monat oder Jahr. **(4 Punkte)** ✓\n",
    "    - Möglichkeit 1: Man spaltet das auf in day(short), month(short) und year(short)\n",
    "    - Möglichkeit 2: Man benutzt das date objekt. allerdings konnte ich hier nicht\n",
    "      herausfinden ob das vernünftig aufgespalten werden kann, deshalb:\n",
    "    - Ich hab mich für 1 entschieden :D\n",
    "    \n",
    "    \n",
    "- 4) Eine Gruppierung nach Dateiendungen. **(2 Punkte)** ✓\n",
    "\n",
    "#### Abgabe\n",
    " - Als Abgabe zählt das Vorzeigen oder Zuschicken des erzeugten Medium- Mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"acknowledged\" : true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Delete existing Index\n",
    "curl -XDELETE 'localhost:9200/htwplus?pretty'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Eine Gruppierung nach Eigentümern (Vor- und Nachname)\n",
    "\n",
    "Wir wollen später Aggregieren können:\n",
    "\n",
    "    curl -XPOST 'localhost:9200/htwplus/medium/_search?size=0&pretty' -H 'Content-Type: application/json' -d'\n",
    "    {\n",
    "      \"aggs\": {\n",
    "        \"by_owner\": {\n",
    "          \"filter\": { \"keyword\": {\"ownername\": \"Iven Zimmer\" } }\n",
    "        }\n",
    "      }\n",
    "    }'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare Index JSON\n",
    "INDEX='\n",
    "{\n",
    "  \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"analyzer\": {\n",
    "        \"edgengram_analyzer\": {\n",
    "          \"tokenizer\": \"htw_edge_ngram\"\n",
    "        },\n",
    "        \"filepath_analyzer\" : {\n",
    "            \"tokenizer\" : \"path_hierarchy\"\n",
    "        }\n",
    "      },\n",
    "      \"tokenizer\": {\n",
    "        \"htw_edge_ngram\": {\n",
    "          \"type\": \"edge_ngram\",\n",
    "          \"min_gram\": 2,\n",
    "          \"max_gram\": 8,\n",
    "          \"token_chars\": [\n",
    "            \"letter\",\n",
    "            \"digit\"\n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"medium\": {\n",
    "      \"properties\": {\n",
    "        \"owner\": {\n",
    "          \"type\": \"short\"\n",
    "        },\n",
    "        \"ownername\": {\n",
    "            \"type\": \"keyword\"\n",
    "        },\n",
    "        \"viewable\": {\n",
    "          \"type\": \"long\"\n",
    "        },\n",
    "        \"public\": {\n",
    "          \"type\": \"boolean\"\n",
    "        },\n",
    "        \"filepath\": {\n",
    "          \"type\": \"text\",\n",
    "          \"analyzer\": \"filepath_analyzer\",\n",
    "          \"search_analyzer\": \"filepath_analyzer\"\n",
    "        },\n",
    "        \"filename\": {\n",
    "          \"type\": \"text\",\n",
    "          \"analyzer\": \"edgengram_analyzer\",\n",
    "          \"search_analyzer\": \"whitespace\"\n",
    "        },\n",
    "        \"filetype\": {\n",
    "            \"type\": \"text\"\n",
    "        },\n",
    "        \"upload_day\": {\n",
    "            \"type\": \"short\"\n",
    "        },\n",
    "        \"upload_month\": {\n",
    "            \"type\": \"short\"\n",
    "        },\n",
    "        \"upload_year\": {\n",
    "            \"type\": \"short\"\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    \"users\": {\n",
    "      \"properties\": {\n",
    "        \"studycourse\": {\n",
    "          \"index\": \"not_analyzed\",\n",
    "          \"type\": \"text\"\n",
    "        },\n",
    "        \"role\": {\n",
    "          \"index\": \"not_analyzed\",\n",
    "          \"type\": \"text\"\n",
    "        },\n",
    "        \"public\": {\n",
    "          \"type\": \"boolean\"\n",
    "        },\n",
    "        \"initial\": {\n",
    "          \"type\": \"keyword\"\n",
    "        },\n",
    "        \"degree\": {\n",
    "          \"index\": \"not_analyzed\",\n",
    "          \"type\": \"text\"\n",
    "        },\n",
    "        \"firstname\": {\n",
    "          \"search_analyzer\": \"whitespace\",\n",
    "          \"analyzer\": \"edgengram_analyzer\",\n",
    "          \"type\": \"text\"\n",
    "        },\n",
    "        \"lastname\": {\n",
    "          \"search_analyzer\": \"whitespace\",\n",
    "          \"analyzer\": \"edgengram_analyzer\",\n",
    "          \"type\": \"text\"\n",
    "        },\n",
    "        \"semester\": {\n",
    "          \"index\": \"not_analyzed\",\n",
    "          \"type\": \"text\"\n",
    "        },\n",
    "        \"avatar\": {\n",
    "          \"type\": \"text\"\n",
    "        },\n",
    "        \"friends\": {\n",
    "          \"type\": \"long\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"acknowledged\" : true,\n",
      "  \"shards_acknowledged\" : true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# => Medium\n",
    "# name => firstname, lastname\n",
    "# The string field is unsupported for indexes created in 5.x in favor of the text and keyword fields.\n",
    "# change all 'string' => 'text'\n",
    "\n",
    "# Since 2.4 the Edge-N-Gram has changed so we create a new custom analyzer for Version 5\n",
    "# This needs to happen at the same time we create the mapping for our type(s)\n",
    "# Otherwise ES does not know about the analyzer ( analyzer not found exception )\n",
    "\n",
    "# After that we Setup The Mapping for Our Users which now include 'firstname' + 'lastname'\n",
    "# Instead of 'name'\n",
    "curl -XPUT 'localhost:9200/htwplus?pretty' -H 'Content-Type: application/json' -d \"${INDEX}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"tokens\" : [\n",
      "    {\n",
      "      \"token\" : \"Iv\",\n",
      "      \"start_offset\" : 0,\n",
      "      \"end_offset\" : 2,\n",
      "      \"type\" : \"word\",\n",
      "      \"position\" : 0\n",
      "    },\n",
      "    {\n",
      "      \"token\" : \"Ive\",\n",
      "      \"start_offset\" : 0,\n",
      "      \"end_offset\" : 3,\n",
      "      \"type\" : \"word\",\n",
      "      \"position\" : 1\n",
      "    },\n",
      "    {\n",
      "      \"token\" : \"Iven\",\n",
      "      \"start_offset\" : 0,\n",
      "      \"end_offset\" : 4,\n",
      "      \"type\" : \"word\",\n",
      "      \"position\" : 2\n",
      "    },\n",
      "    {\n",
      "      \"token\" : \"Zi\",\n",
      "      \"start_offset\" : 5,\n",
      "      \"end_offset\" : 7,\n",
      "      \"type\" : \"word\",\n",
      "      \"position\" : 3\n",
      "    },\n",
      "    {\n",
      "      \"token\" : \"Zim\",\n",
      "      \"start_offset\" : 5,\n",
      "      \"end_offset\" : 8,\n",
      "      \"type\" : \"word\",\n",
      "      \"position\" : 4\n",
      "    },\n",
      "    {\n",
      "      \"token\" : \"Zimm\",\n",
      "      \"start_offset\" : 5,\n",
      "      \"end_offset\" : 9,\n",
      "      \"type\" : \"word\",\n",
      "      \"position\" : 5\n",
      "    },\n",
      "    {\n",
      "      \"token\" : \"Zimme\",\n",
      "      \"start_offset\" : 5,\n",
      "      \"end_offset\" : 10,\n",
      "      \"type\" : \"word\",\n",
      "      \"position\" : 6\n",
      "    },\n",
      "    {\n",
      "      \"token\" : \"Zimmer\",\n",
      "      \"start_offset\" : 5,\n",
      "      \"end_offset\" : 11,\n",
      "      \"type\" : \"word\",\n",
      "      \"position\" : 7\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Test The analyzer for User (Not really necessary, but I wanted to test that.)\n",
    "curl -XPOST 'localhost:9200/htwplus/_analyze?pretty' -H 'Content-Type: application/json' -d'\n",
    "{\n",
    "  \"analyzer\": \"edgengram_analyzer\",\n",
    "  \"text\": [\"Iven Zimmer\"]\n",
    "}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"tokens\" : [\n",
      "    {\n",
      "      \"token\" : \"/Users\",\n",
      "      \"start_offset\" : 0,\n",
      "      \"end_offset\" : 6,\n",
      "      \"type\" : \"word\",\n",
      "      \"position\" : 0\n",
      "    },\n",
      "    {\n",
      "      \"token\" : \"/Users/zimmeri\",\n",
      "      \"start_offset\" : 0,\n",
      "      \"end_offset\" : 14,\n",
      "      \"type\" : \"word\",\n",
      "      \"position\" : 0\n",
      "    },\n",
      "    {\n",
      "      \"token\" : \"/Users/zimmeri/elastic17\",\n",
      "      \"start_offset\" : 0,\n",
      "      \"end_offset\" : 24,\n",
      "      \"type\" : \"word\",\n",
      "      \"position\" : 0\n",
      "    },\n",
      "    {\n",
      "      \"token\" : \"/Users/zimmeri/elastic17/Klausur.pdf\",\n",
      "      \"start_offset\" : 0,\n",
      "      \"end_offset\" : 36,\n",
      "      \"type\" : \"word\",\n",
      "      \"position\" : 0\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "curl -XPOST 'localhost:9200/htwplus/_analyze?pretty' -H 'Content-Type: application/json' -d'\n",
    "{\n",
    "  \"analyzer\": \"filepath_analyzer\",\n",
    "  \"text\": [\"/Users/zimmeri/elastic17/Klausur.pdf\"]\n",
    "}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
